Running simulation
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Done loading model and tokenizer!
Done preprocessing dataset!
/gpfs/bwfor/home/tu/tu_tu/tu_zxojp43/master_thesis/RLHF/reward_model.py:97: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  sampled = grouped.apply(lambda x: x.sample(min(len(x), sample_size), random_state=42))
Reward Logits:  tensor([1.3594], dtype=torch.bfloat16)
Reward Logits:  tensor([2.0469], dtype=torch.bfloat16)
Reward Logits:  tensor([1.1328], dtype=torch.bfloat16)
Reward Logits:  tensor([1.5859], dtype=torch.bfloat16)
Reward Logits:  tensor([2.6719], dtype=torch.bfloat16)
Reward Logits:  tensor([1.5391], dtype=torch.bfloat16)
Reward Logits:  tensor([0.9219], dtype=torch.bfloat16)
Reward Logits:  tensor([2.8594], dtype=torch.bfloat16)
Reward Logits:  tensor([-1.2812], dtype=torch.bfloat16)
Reward Logits:  tensor([2.2500], dtype=torch.bfloat16)
Reward Logits:  tensor([1.1172], dtype=torch.bfloat16)
Reward Logits:  tensor([2.6094], dtype=torch.bfloat16)
Reward Logits:  tensor([1.1172], dtype=torch.bfloat16)
Reward Logits:  tensor([3.5000], dtype=torch.bfloat16)
Reward Logits:  tensor([1.1172], dtype=torch.bfloat16)
Reward Logits:  tensor([4.6562], dtype=torch.bfloat16)
Reward Logits:  tensor([1.1172], dtype=torch.bfloat16)
Reward Logits:  tensor([-0.0332], dtype=torch.bfloat16)
Reward Logits:  tensor([1.1172], dtype=torch.bfloat16)
Reward Logits:  tensor([7.4062], dtype=torch.bfloat16)
Reward Logits:  tensor([2.5469], dtype=torch.bfloat16)
Reward Logits:  tensor([5.4375], dtype=torch.bfloat16)
Reward Logits:  tensor([1.5000], dtype=torch.bfloat16)
Reward Logits:  tensor([2.8281], dtype=torch.bfloat16)
Reward Logits:  tensor([1.9688], dtype=torch.bfloat16)
Reward Logits:  tensor([2.7344], dtype=torch.bfloat16)
Reward Logits:  tensor([0.9531], dtype=torch.bfloat16)
Reward Logits:  tensor([1.3828], dtype=torch.bfloat16)
Reward Logits:  tensor([0.3672], dtype=torch.bfloat16)
Reward Logits:  tensor([5.1875], dtype=torch.bfloat16)
Reward Logits:  tensor([3.5625], dtype=torch.bfloat16)
Reward Logits:  tensor([2.6875], dtype=torch.bfloat16)
Reward Logits:  tensor([2.8594], dtype=torch.bfloat16)
Reward Logits:  tensor([2.4531], dtype=torch.bfloat16)
Reward Logits:  tensor([3.2031], dtype=torch.bfloat16)
Reward Logits:  tensor([2.6406], dtype=torch.bfloat16)
Reward Logits:  tensor([3.4531], dtype=torch.bfloat16)
Reward Logits:  tensor([2.5312], dtype=torch.bfloat16)
Reward Logits:  tensor([3.2500], dtype=torch.bfloat16)
Reward Logits:  tensor([2.5156], dtype=torch.bfloat16)
/gpfs/bwfor/home/tu/tu_tu/tu_zxojp43/master_thesis/RLHF/reward_model.py:97: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  sampled = grouped.apply(lambda x: x.sample(min(len(x), sample_size), random_state=42))
Reward Logits:  tensor([1.9473])
Reward Logits:  tensor([2.4854])
Reward Logits:  tensor([0.9624])
Reward Logits:  tensor([1.4316])
Reward Logits:  tensor([2.5948])
Reward Logits:  tensor([1.2959])
Reward Logits:  tensor([1.3518])
Reward Logits:  tensor([2.0033])
Reward Logits:  tensor([-1.3553])
Reward Logits:  tensor([1.9981])
Reward Logits:  tensor([1.3401])
Reward Logits:  tensor([2.7660])
Reward Logits:  tensor([1.3401])
Reward Logits:  tensor([3.6489])
Reward Logits:  tensor([1.3401])
Reward Logits:  tensor([5.0228])
Reward Logits:  tensor([1.3401])
Reward Logits:  tensor([0.4052])
Reward Logits:  tensor([1.3401])
Reward Logits:  tensor([7.4454])
Reward Logits:  tensor([1.8809])
Reward Logits:  tensor([6.3045])
Reward Logits:  tensor([0.4461])
Reward Logits:  tensor([2.6993])
Reward Logits:  tensor([1.3910])
Reward Logits:  tensor([2.4821])
Reward Logits:  tensor([0.8826])
Reward Logits:  tensor([1.1501])
Reward Logits:  tensor([1.0167])
Reward Logits:  tensor([5.8078])
Reward Logits:  tensor([3.4701])
Reward Logits:  tensor([2.1706])
Reward Logits:  tensor([2.7964])
Reward Logits:  tensor([2.4639])
Reward Logits:  tensor([3.1264])
Reward Logits:  tensor([1.8338])
Reward Logits:  tensor([3.1312])
Reward Logits:  tensor([2.3301])
Reward Logits:  tensor([2.5709])
Reward Logits:  tensor([2.2584])
