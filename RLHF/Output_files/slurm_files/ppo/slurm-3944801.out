Running simulation
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:04<00:20,  4.13s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:08<00:16,  4.07s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:12<00:12,  4.20s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:16<00:08,  4.16s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:20<00:04,  4.23s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:22<00:00,  3.18s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:22<00:00,  3.68s/it]
WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from '../SFT/merged_model/SFT_for_expert_alignment/', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Done loading Policy Model and Tokenizer!
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Done loading Reward Model and Tokenizer!
Map:   0%|          | 0/17427 [00:00<?, ? examples/s]Map:   1%|▏         | 253/17427 [00:00<00:06, 2518.44 examples/s]Map:   3%|▎         | 517/17427 [00:00<00:06, 2582.38 examples/s]Map:   5%|▍         | 787/17427 [00:00<00:06, 2629.94 examples/s]Map:   6%|▋         | 1129/17427 [00:00<00:07, 2313.98 examples/s]Map:   8%|▊         | 1387/17427 [00:00<00:06, 2392.58 examples/s]Map:  10%|▉         | 1661/17427 [00:00<00:06, 2495.60 examples/s]Map:  11%|█         | 1923/17427 [00:00<00:06, 2528.21 examples/s]Map:  13%|█▎        | 2259/17427 [00:00<00:06, 2318.64 examples/s]Map:  15%|█▍        | 2528/17427 [00:01<00:06, 2414.06 examples/s]Map:  16%|█▌        | 2802/17427 [00:01<00:05, 2500.52 examples/s]Map:  18%|█▊        | 3133/17427 [00:01<00:06, 2355.16 examples/s]Map:  20%|█▉        | 3399/17427 [00:01<00:05, 2432.55 examples/s]Map:  21%|██        | 3674/17427 [00:01<00:05, 2514.10 examples/s]Map:  23%|██▎       | 3947/17427 [00:01<00:05, 2571.06 examples/s]Map:  25%|██▍       | 4274/17427 [00:01<00:05, 2392.75 examples/s]Map:  26%|██▌       | 4539/17427 [00:01<00:05, 2457.23 examples/s]Map:  28%|██▊       | 4811/17427 [00:01<00:05, 2523.05 examples/s]Map:  29%|██▉       | 5112/17427 [00:02<00:05, 2265.19 examples/s]Map:  31%|███       | 5376/17427 [00:02<00:05, 2353.78 examples/s]Map:  32%|███▏      | 5650/17427 [00:02<00:04, 2455.73 examples/s]Map:  34%|███▍      | 6000/17427 [00:02<00:05, 2201.80 examples/s]Map:  36%|███▌      | 6258/17427 [00:02<00:04, 2291.58 examples/s]Map:  37%|███▋      | 6526/17427 [00:02<00:04, 2387.32 examples/s]Map:  39%|███▉      | 6790/17427 [00:02<00:04, 2448.52 examples/s]Map:  41%|████      | 7119/17427 [00:02<00:04, 2270.07 examples/s]Map:  42%|████▏     | 7390/17427 [00:03<00:04, 2378.76 examples/s]Map:  44%|████▍     | 7652/17427 [00:03<00:04, 2436.60 examples/s]Map:  45%|████▌     | 7928/17427 [00:03<00:03, 2519.95 examples/s]Map:  47%|████▋     | 8258/17427 [00:03<00:03, 2329.65 examples/s]Map:  49%|████▉     | 8514/17427 [00:03<00:03, 2386.74 examples/s]Map:  50%|█████     | 8768/17427 [00:03<00:03, 2421.17 examples/s]Map:  52%|█████▏    | 9130/17427 [00:03<00:03, 2277.24 examples/s]Map:  54%|█████▍    | 9409/17427 [00:03<00:03, 2401.15 examples/s]Map:  55%|█████▌    | 9667/17427 [00:04<00:03, 2444.57 examples/s]Map:  57%|█████▋    | 9931/17427 [00:04<00:03, 2494.51 examples/s]Map:  59%|█████▉    | 10246/17427 [00:04<00:03, 2272.94 examples/s]Map:  60%|██████    | 10501/17427 [00:04<00:02, 2338.58 examples/s]Map:  62%|██████▏   | 10765/17427 [00:04<00:02, 2416.96 examples/s]Map:  64%|██████▍   | 11124/17427 [00:04<00:02, 2278.81 examples/s]Map:  65%|██████▌   | 11391/17427 [00:04<00:02, 2371.84 examples/s]Map:  67%|██████▋   | 11641/17427 [00:04<00:02, 2400.60 examples/s]Map:  68%|██████▊   | 11903/17427 [00:04<00:02, 2456.15 examples/s]Map:  70%|███████   | 12266/17427 [00:05<00:02, 2273.91 examples/s]Map:  72%|███████▏  | 12520/17427 [00:05<00:02, 2334.68 examples/s]Map:  73%|███████▎  | 12782/17427 [00:05<00:01, 2406.12 examples/s]Map:  75%|███████▌  | 13131/17427 [00:05<00:01, 2288.29 examples/s]Map:  77%|███████▋  | 13389/17427 [00:05<00:01, 2358.59 examples/s]Map:  78%|███████▊  | 13659/17427 [00:05<00:01, 2444.90 examples/s]Map:  80%|████████  | 13947/17427 [00:05<00:01, 2560.07 examples/s]Map:  82%|████████▏ | 14260/17427 [00:05<00:01, 2353.81 examples/s]Map:  83%|████████▎ | 14528/17427 [00:06<00:01, 2434.71 examples/s]Map:  85%|████████▍ | 14797/17427 [00:06<00:01, 2501.35 examples/s]Map:  87%|████████▋ | 15129/17427 [00:06<00:00, 2377.20 examples/s]Map:  88%|████████▊ | 15387/17427 [00:06<00:00, 2427.96 examples/s]Map:  90%|████████▉ | 15671/17427 [00:06<00:00, 2536.94 examples/s]Map:  91%|█████████▏| 15935/17427 [00:06<00:00, 2564.03 examples/s]Map:  93%|█████████▎| 16268/17427 [00:06<00:00, 2385.99 examples/s]Map:  95%|█████████▍| 16530/17427 [00:06<00:00, 2444.07 examples/s]Map:  96%|█████████▋| 16798/17427 [00:06<00:00, 2505.85 examples/s]Map:  98%|█████████▊| 17129/17427 [00:07<00:00, 2337.82 examples/s]Map: 100%|█████████▉| 17400/17427 [00:07<00:00, 2429.65 examples/s]Map: 100%|██████████| 17427/17427 [00:07<00:00, 2399.37 examples/s]
epoch:   0%|          | 0/1 [00:00<?, ?it/s]
  0%|          | 0/8713 [00:00<?, ?it/s][AQuery Tensors:  2
Query Tensors:  [tensor([    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            1,     2,     1,   518, 25580, 29962,  3532, 14816, 29903,  6778,
           13, 12148,  1234,   278,  5155,  4475,   304,  1737,   359, 15277,
        29889,    13, 29966,   829, 14816, 29903,  6778,    13, 12148,  6597,
          278,  4257, 16212,   297, 29901,   450,   937, 11340, 21983,   309,
        10943,   515,   278, 24929, 16081,   465,   293, 10288, 29890,  1431,
          279, 12030, 14195,   297,  8314,   338,  5439,   322, 26848, 29889,
        16564,   373,   263,  2323, 29892,  1532, 21634, 21612, 29892,  3037,
          359,  1191,   375,   313,  7856,   359,  1191,   375, 29897,  5969,
        29890,  1431,   279,  6322,   716,  6606,   338,  9859,   304,   278,
         1294,  5562,  4105, 23766,  2265,   273,  3942,  2595,   275,  1191,
         3898, 29889, 27792, 18131,  3002, 14661,   263,  9443,  1546,   445,
         1294,  5562,  3942,   322,  3767,  2219, 16103,  3898, 29889,   450,
         2595,   275,  1191,  3898,   505,   577,  2215,   871,  1063,  1476,
          297,  7551, 29892, 15198, 19426, 14411,   322, 21952, 18001, 29889,
         3037,   359,  1191,   375,  7849,   586,  2574, 29892, 29871, 29896,
        29929, 29929, 29955,  5279,  7199,  4637,  9881,  6606,   515,   278,
         3037,  5590,   713,   313,  9632, 14253, 16081,   465,   293, 29897,
          310,  7551,   322,  2211,  6606,   515,   278, 11045,   713,   313,
          799,   368, 24929, 16081,   465,   293, 29897,   310, 15198, 19426,
        14411, 29889,   910, 23947,  4105, 23766,  2265,   273, 21983,   309,
          515,  8314,   322,   937,  2407,   310,   278,  2595,   275,  1191,
         3898,   297,   278, 14841,  9736,   275,  9085, 22981, 10757,   393,
         6133,   285,  3687,   892,  2198,   297,   445,  5120,   297,   278,
        16081,   465,   293, 29892,   746,   445,  2318,   338,  2714,   304,
          505,   844,  9223,   967,  6894,  2450,   636, 29871,   518, 29914,
        25580, 29962], device='cuda:0'), tensor([    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     1,     2,     1,   518,
        25580, 29962,  3532, 14816, 29903,  6778,    13, 12148,  1234,   278,
         5155,  4475,   304,  1737,   359, 15277, 29889,    13, 29966,   829,
        14816, 29903,  6778,    13, 25125,  5019,   505,  7271, 14338,   385,
         3415, 26191,   568, 10856,   303,  4978,  1904, 29973,    13, 29902,
          626,  1985,   411,   263,  8368,   304,  1904,   278,  4978,   310,
        10856,   303,   297, 16216, 11032,   423, 29892,  7400, 29892,   988,
         1556, 10856,   303,   338,  1090,  7420,   491, 10966, 15663, 29889,
         1334,   864,   304,  2910, 10161,   411, 17768,   403,   304,  1880,
         6976,   310,  7101, 10856,   303,  5849,   313,   705, 29915,   276,
        19434,  8852,   297, 28169, 29899, 29716, 27170,   313, 29881,   324,
         1475,  8106,   512,  6124,   304,  6592, 20821,  1737,  3002, 29892,
        13879,   393,   591,   526, 13858,  3160, 10809,   304,  4094,  1591,
          313,  1457, 16604,  1904,   839,   511,   975,  8399,  1145, 10809,
        29892,   322, 22473,  1277,   359,   537,   313,  2248,   287,   491,
         1273,   310,  1067,   388,   297, 22473,   467,  5538,  5019,   505,
         7271,   411,   445,  2924,   310,  5925, 29973,  1334,   864,   304,
         1831,  8210,  4128,   363,  1269,   310,  1438,  3651,   313, 29872,
        29889, 29887,  1696,   825, 10809,   310,   975,  8399,  1145, 29892,
        10809,   304,  4094,  1591, 29892,  2992,  6250, 29871,   518, 29914,
        25580, 29962], device='cuda:0')]
Response Tensors:  [tensor([29889, 15993,   786, 29901, 20281, 14995, 27794,  5953, 30889,   660,
         6143, 29872,   398, 24709,   273,   498, 14995, 29906, 12729, 10943],
       device='cuda:0'), tensor([31872,     1,   718, 25214, 10541,   317, 20668,  7858,  5192,  2477,
         1516, 29888,  5019,  1123, 10457, 23134,  7967, 11082,  2025,  8446],
       device='cuda:0')]
Response Tensors:  2
Reward Logits:  tensor([0.6516], device='cuda:0')
Reward Logits:  tensor([0.9976], device='cuda:0')
Rewards List:  [tensor(0.6516, device='cuda:0'), tensor(0.9976, device='cuda:0')]
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Stats:  {'objective/kl': 0.0, 'objective/kl_dist': array([0., 0.], dtype=float32), 'objective/logprobs': array([[-22.02601  , -22.02601  , -22.02601  , ...,  -4.9557056,
        -10.674408 , -10.631025 ],
       [-22.02601  , -22.02601  , -22.02601  , ..., -10.8352375,
        -10.096785 ,  -9.651636 ]], dtype=float32), 'objective/ref_logprobs': array([[-22.02601  , -22.02601  , -22.02601  , ...,  -4.9557056,
        -10.674408 , -10.631025 ],
       [-22.02601  , -22.02601  , -22.02601  , ..., -10.8352375,
        -10.096785 ,  -9.651636 ]], dtype=float32), 'objective/kl_coef': 0.2, 'objective/entropy': 176.74256896972656, 'ppo/mean_non_score_reward': 0.0, 'ppo/mean_scores': 0.8246092200279236, 'ppo/std_scores': 0.2447112649679184, 'tokens/queries_len_mean': 512.0, 'tokens/queries_len_std': 0.0, 'tokens/queries_dist': array([512., 512.], dtype=float32), 'tokens/responses_len_mean': 20.0, 'tokens/responses_len_std': 0.0, 'tokens/responses_dist': array([20., 20.], dtype=float32), 'ppo/loss/policy': nan, 'ppo/loss/value': nan, 'ppo/loss/total': nan, 'ppo/policy/entropy': nan, 'ppo/policy/approxkl': nan, 'ppo/policy/policykl': nan, 'ppo/policy/clipfrac': 0.0, 'ppo/policy/advantages': array([-3.7328184, -3.7328184, -3.7328184, ...,  0.6484721,  1.9656414,
        1.1948361], dtype=float32), 'ppo/policy/advantages_mean': -1.8030405612989853e-07, 'ppo/policy/ratio': array([ 1.,  1.,  1., ..., nan, nan, nan], dtype=float32), 'ppo/returns/mean': 0.3291398286819458, 'ppo/returns/var': 0.07396463304758072, 'ppo/val/vpred': nan, 'ppo/val/error': nan, 'ppo/val/clipfrac': 0.02500000037252903, 'ppo/val/mean': -0.5738201141357422, 'ppo/val/var': 0.021302644163370132, 'ppo/time/ppo/optimizer_step': 0.10028243064880371, 'ppo/val/var_explained': nan, 'ppo/learning_rate': 2e-05, 'time/ppo/forward_pass': 1.5870616436004639, 'time/ppo/compute_rewards': 0.0784761905670166, 'time/ppo/optimize_step': 2.2050607204437256, 'time/ppo/calc_stats': 0.09051299095153809, 'time/ppo/total': 4.00934624671936}
  0%|          | 0/8713 [00:06<?, ?it/s]
epoch:   0%|          | 0/1 [00:06<?, ?it/s]
Traceback (most recent call last):
  File "/gpfs/bwfor/home/tu/tu_tu/tu_zxojp43/master_thesis/RLHF/ppo.py", line 184, in <module>
    build_pipeline(ppo_config, ppo_trainer, policy_tokenizer, reward_model, reward_tokenizer, dataloader)
  File "/gpfs/bwfor/home/tu/tu_tu/tu_zxojp43/master_thesis/RLHF/ppo.py", line 142, in build_pipeline
    ppo_trainer.log_stats(stats, batch, rewards)
NameError: name 'rewards' is not defined
