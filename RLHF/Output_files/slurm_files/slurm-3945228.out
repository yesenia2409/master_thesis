Running simulation
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:04<00:22,  4.46s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:08<00:17,  4.49s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:13<00:13,  4.60s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:18<00:09,  4.75s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:24<00:04,  4.98s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:25<00:00,  3.74s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:25<00:00,  4.23s/it]
WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from '../SFT/merged_model/SFT_for_expert_alignment/', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Done loading Policy Model and Tokenizer!
/home/tu/tu_tu/tu_zxojp43/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Done loading Reward Model and Tokenizer!
WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home/tu/tu_tu/tu_zxojp43/.local/lib/python3.9/site-packages/transformers/quantizers/auto.py:155: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.19s/it]
Some weights of the model checkpoint at Policy_Model/ were not used when initializing LlamaForCausalLM: ['v_head.summary.bias', 'v_head.summary.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from 'Policy_Model/', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Re-loading model done!
