Running simulation
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Done loading model and tokenizer!
Done preprocessing dataset!
/gpfs/bwfor/home/tu/tu_tu/tu_zxojp43/master_thesis/RLHF/reward_model.py:97: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  sampled = grouped.apply(lambda x: x.sample(min(len(x), sample_size), random_state=5))
Reward Logits:  tensor([0.7695], dtype=torch.bfloat16)
Reward Logits:  tensor([0.0110], dtype=torch.bfloat16)
Reward Logits:  tensor([-0.3398], dtype=torch.bfloat16)
Reward Logits:  tensor([2.5312], dtype=torch.bfloat16)
Reward Logits:  tensor([-1.2812], dtype=torch.bfloat16)
Reward Logits:  tensor([2.2500], dtype=torch.bfloat16)
Reward Logits:  tensor([2.6719], dtype=torch.bfloat16)
Reward Logits:  tensor([1.5391], dtype=torch.bfloat16)
Reward Logits:  tensor([1.8750], dtype=torch.bfloat16)
Reward Logits:  tensor([2.0312], dtype=torch.bfloat16)
Reward Logits:  tensor([1.1172], dtype=torch.bfloat16)
Reward Logits:  tensor([1.7891], dtype=torch.bfloat16)
Reward Logits:  tensor([1.1172], dtype=torch.bfloat16)
Reward Logits:  tensor([3.9844], dtype=torch.bfloat16)
Reward Logits:  tensor([1.1172], dtype=torch.bfloat16)
Reward Logits:  tensor([2.2500], dtype=torch.bfloat16)
Reward Logits:  tensor([1.1172], dtype=torch.bfloat16)
Reward Logits:  tensor([0.7500], dtype=torch.bfloat16)
Reward Logits:  tensor([1.1172], dtype=torch.bfloat16)
Reward Logits:  tensor([3.8125], dtype=torch.bfloat16)
Reward Logits:  tensor([0.3086], dtype=torch.bfloat16)
Reward Logits:  tensor([5.1562], dtype=torch.bfloat16)
Reward Logits:  tensor([1.9531], dtype=torch.bfloat16)
Reward Logits:  tensor([1.7266], dtype=torch.bfloat16)
Reward Logits:  tensor([2.3750], dtype=torch.bfloat16)
Reward Logits:  tensor([4.3750], dtype=torch.bfloat16)
Reward Logits:  tensor([0.9414], dtype=torch.bfloat16)
Reward Logits:  tensor([2.5156], dtype=torch.bfloat16)
Reward Logits:  tensor([0.9609], dtype=torch.bfloat16)
Reward Logits:  tensor([0.2324], dtype=torch.bfloat16)
Reward Logits:  tensor([3.5625], dtype=torch.bfloat16)
Reward Logits:  tensor([2.6875], dtype=torch.bfloat16)
Reward Logits:  tensor([3.1406], dtype=torch.bfloat16)
Reward Logits:  tensor([1.5000], dtype=torch.bfloat16)
Reward Logits:  tensor([3.9531], dtype=torch.bfloat16)
Reward Logits:  tensor([2.6875], dtype=torch.bfloat16)
Reward Logits:  tensor([2.3281], dtype=torch.bfloat16)
Reward Logits:  tensor([2.1250], dtype=torch.bfloat16)
Reward Logits:  tensor([2.7188], dtype=torch.bfloat16)
Reward Logits:  tensor([1.8594], dtype=torch.bfloat16)
/gpfs/bwfor/home/tu/tu_tu/tu_zxojp43/master_thesis/RLHF/reward_model.py:97: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  sampled = grouped.apply(lambda x: x.sample(min(len(x), sample_size), random_state=5))
Reward Logits:  tensor([0.5689])
Reward Logits:  tensor([0.2136])
Reward Logits:  tensor([-0.2769])
Reward Logits:  tensor([2.6751])
Reward Logits:  tensor([-1.3553])
Reward Logits:  tensor([1.9981])
Reward Logits:  tensor([2.5948])
Reward Logits:  tensor([1.2959])
Reward Logits:  tensor([1.7485])
Reward Logits:  tensor([1.9177])
Reward Logits:  tensor([1.3401])
Reward Logits:  tensor([2.1711])
Reward Logits:  tensor([1.3401])
Reward Logits:  tensor([4.3405])
Reward Logits:  tensor([1.3401])
Reward Logits:  tensor([1.9768])
Reward Logits:  tensor([1.3401])
Reward Logits:  tensor([1.3574])
Reward Logits:  tensor([1.3401])
Reward Logits:  tensor([4.1622])
Reward Logits:  tensor([0.5281])
Reward Logits:  tensor([5.0164])
Reward Logits:  tensor([2.0655])
Reward Logits:  tensor([2.0096])
Reward Logits:  tensor([2.5099])
Reward Logits:  tensor([4.2143])
Reward Logits:  tensor([0.6978])
Reward Logits:  tensor([2.4730])
Reward Logits:  tensor([0.7033])
Reward Logits:  tensor([0.2542])
Reward Logits:  tensor([3.4701])
Reward Logits:  tensor([2.1706])
Reward Logits:  tensor([3.4817])
Reward Logits:  tensor([1.3851])
Reward Logits:  tensor([3.9417])
Reward Logits:  tensor([2.3610])
Reward Logits:  tensor([2.6406])
Reward Logits:  tensor([1.6281])
Reward Logits:  tensor([3.0283])
Reward Logits:  tensor([1.1952])
