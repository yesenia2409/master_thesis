Running simulation
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:04<00:22,  4.55s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:09<00:18,  4.52s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:16<00:17,  5.68s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:20<00:10,  5.29s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:25<00:05,  5.11s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:26<00:00,  3.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:26<00:00,  4.47s/it]
WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from '../SFT/merged_model/SFT_for_expert_alignment/', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Done loading Policy Model and Tokenizer!
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Done loading Reward Model and Tokenizer!
Map:   0%|          | 0/17427 [00:00<?, ? examples/s]Map:   1%|â–         | 252/17427 [00:00<00:06, 2503.00 examples/s]Map:   3%|â–Ž         | 514/17427 [00:00<00:06, 2566.45 examples/s]Map:   4%|â–         | 780/17427 [00:00<00:06, 2605.82 examples/s]Map:   6%|â–‹         | 1127/17427 [00:00<00:07, 2295.16 examples/s]Map:   8%|â–Š         | 1380/17427 [00:00<00:06, 2364.68 examples/s]Map:   9%|â–‰         | 1648/17427 [00:00<00:06, 2455.22 examples/s]Map:  11%|â–ˆ         | 1905/17427 [00:00<00:06, 2484.38 examples/s]Map:  13%|â–ˆâ–Ž        | 2253/17427 [00:00<00:06, 2278.50 examples/s]Map:  14%|â–ˆâ–        | 2518/17427 [00:01<00:06, 2370.74 examples/s]Map:  16%|â–ˆâ–Œ        | 2790/17427 [00:01<00:05, 2463.37 examples/s]Map:  18%|â–ˆâ–Š        | 3131/17427 [00:01<00:06, 2322.49 examples/s]Map:  19%|â–ˆâ–‰        | 3392/17427 [00:01<00:05, 2393.11 examples/s]Map:  21%|â–ˆâ–ˆ        | 3662/17427 [00:01<00:05, 2472.45 examples/s]Map:  23%|â–ˆâ–ˆâ–Ž       | 3936/17427 [00:01<00:05, 2539.15 examples/s]Map:  25%|â–ˆâ–ˆâ–       | 4271/17427 [00:01<00:05, 2361.12 examples/s]Map:  26%|â–ˆâ–ˆâ–Œ       | 4533/17427 [00:01<00:05, 2423.94 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 4802/17427 [00:01<00:05, 2492.55 examples/s]Map:  29%|â–ˆâ–ˆâ–‰       | 5115/17427 [00:02<00:05, 2262.48 examples/s]Map:  31%|â–ˆâ–ˆâ–ˆ       | 5379/17427 [00:02<00:05, 2355.55 examples/s]Map:  32%|â–ˆâ–ˆâ–ˆâ–      | 5655/17427 [00:02<00:04, 2460.96 examples/s]Map:  34%|â–ˆâ–ˆâ–ˆâ–      | 6000/17427 [00:02<00:05, 2226.67 examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 6253/17427 [00:02<00:04, 2297.84 examples/s]Map:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 6522/17427 [00:02<00:04, 2393.29 examples/s]Map:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 6786/17427 [00:02<00:04, 2453.22 examples/s]Map:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7118/17427 [00:02<00:04, 2277.11 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7388/17427 [00:03<00:04, 2380.89 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7651/17427 [00:03<00:03, 2445.10 examples/s]Map:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 7925/17427 [00:03<00:03, 2522.88 examples/s]Map:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8256/17427 [00:03<00:03, 2335.34 examples/s]Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 8512/17427 [00:03<00:03, 2387.72 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8763/17427 [00:03<00:03, 2414.19 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9128/17427 [00:03<00:03, 2257.42 examples/s]Map:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9404/17427 [00:03<00:03, 2375.86 examples/s]Map:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9657/17427 [00:04<00:03, 2413.42 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9915/17427 [00:04<00:03, 2452.03 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10244/17427 [00:04<00:03, 2237.50 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 10496/17427 [00:04<00:03, 2304.10 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 10755/17427 [00:04<00:02, 2375.20 examples/s]Map:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 11000/17427 [00:04<00:02, 2222.29 examples/s]Map:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11260/17427 [00:04<00:02, 2321.11 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 11513/17427 [00:04<00:02, 2377.85 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 11764/17427 [00:04<00:02, 2406.46 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 12129/17427 [00:05<00:02, 2212.22 examples/s]Map:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12389/17427 [00:05<00:02, 2297.35 examples/s]Map:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 12643/17427 [00:05<00:02, 2359.90 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 12901/17427 [00:05<00:01, 2418.96 examples/s]Map:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 13265/17427 [00:05<00:01, 2300.64 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 13521/17427 [00:05<00:01, 2363.34 examples/s]Map:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 13789/17427 [00:05<00:01, 2445.51 examples/s]Map:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 14125/17427 [00:05<00:01, 2326.56 examples/s]Map:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 14385/17427 [00:06<00:01, 2391.43 examples/s]Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14658/17427 [00:06<00:01, 2471.13 examples/s]Map:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 14937/17427 [00:06<00:00, 2554.92 examples/s]Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15261/17427 [00:06<00:00, 2368.83 examples/s]Map:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 15529/17427 [00:06<00:00, 2442.07 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 15801/17427 [00:06<00:00, 2513.38 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 16132/17427 [00:06<00:00, 2352.71 examples/s]Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16403/17427 [00:06<00:00, 2438.48 examples/s]Map:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 16663/17427 [00:06<00:00, 2479.30 examples/s]Map:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 16931/17427 [00:07<00:00, 2531.12 examples/s]Map:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 17267/17427 [00:07<00:00, 2349.76 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17427/17427 [00:07<00:00, 2378.70 examples/s]
epoch:   0%|          | 0/1 [00:00<?, ?it/s]
  0%|          | 0/8713 [00:00<?, ?it/s][AQuery Tensors:  2
Query Tensors:  [tensor([    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            1,     2,     1,   518, 25580, 29962,  3532, 14816, 29903,  6778,
           13, 12148,  1234,   278,  5155,  4475,   304,  1737,   359, 15277,
        29889,    13, 29966,   829, 14816, 29903,  6778,    13, 12148,  6597,
          278,  4257, 16212,   297, 29901,   450,   937, 11340, 21983,   309,
        10943,   515,   278, 24929, 16081,   465,   293, 10288, 29890,  1431,
          279, 12030, 14195,   297,  8314,   338,  5439,   322, 26848, 29889,
        16564,   373,   263,  2323, 29892,  1532, 21634, 21612, 29892,  3037,
          359,  1191,   375,   313,  7856,   359,  1191,   375, 29897,  5969,
        29890,  1431,   279,  6322,   716,  6606,   338,  9859,   304,   278,
         1294,  5562,  4105, 23766,  2265,   273,  3942,  2595,   275,  1191,
         3898, 29889, 27792, 18131,  3002, 14661,   263,  9443,  1546,   445,
         1294,  5562,  3942,   322,  3767,  2219, 16103,  3898, 29889,   450,
         2595,   275,  1191,  3898,   505,   577,  2215,   871,  1063,  1476,
          297,  7551, 29892, 15198, 19426, 14411,   322, 21952, 18001, 29889,
         3037,   359,  1191,   375,  7849,   586,  2574, 29892, 29871, 29896,
        29929, 29929, 29955,  5279,  7199,  4637,  9881,  6606,   515,   278,
         3037,  5590,   713,   313,  9632, 14253, 16081,   465,   293, 29897,
          310,  7551,   322,  2211,  6606,   515,   278, 11045,   713,   313,
          799,   368, 24929, 16081,   465,   293, 29897,   310, 15198, 19426,
        14411, 29889,   910, 23947,  4105, 23766,  2265,   273, 21983,   309,
          515,  8314,   322,   937,  2407,   310,   278,  2595,   275,  1191,
         3898,   297,   278, 14841,  9736,   275,  9085, 22981, 10757,   393,
         6133,   285,  3687,   892,  2198,   297,   445,  5120,   297,   278,
        16081,   465,   293, 29892,   746,   445,  2318,   338,  2714,   304,
          505,   844,  9223,   967,  6894,  2450,   636, 29871,   518, 29914,
        25580, 29962], device='cuda:0'), tensor([    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     1,     2,     1,   518,
        25580, 29962,  3532, 14816, 29903,  6778,    13, 12148,  1234,   278,
         5155,  4475,   304,  1737,   359, 15277, 29889,    13, 29966,   829,
        14816, 29903,  6778,    13, 25125,  5019,   505,  7271, 14338,   385,
         3415, 26191,   568, 10856,   303,  4978,  1904, 29973,    13, 29902,
          626,  1985,   411,   263,  8368,   304,  1904,   278,  4978,   310,
        10856,   303,   297, 16216, 11032,   423, 29892,  7400, 29892,   988,
         1556, 10856,   303,   338,  1090,  7420,   491, 10966, 15663, 29889,
         1334,   864,   304,  2910, 10161,   411, 17768,   403,   304,  1880,
         6976,   310,  7101, 10856,   303,  5849,   313,   705, 29915,   276,
        19434,  8852,   297, 28169, 29899, 29716, 27170,   313, 29881,   324,
         1475,  8106,   512,  6124,   304,  6592, 20821,  1737,  3002, 29892,
        13879,   393,   591,   526, 13858,  3160, 10809,   304,  4094,  1591,
          313,  1457, 16604,  1904,   839,   511,   975,  8399,  1145, 10809,
        29892,   322, 22473,  1277,   359,   537,   313,  2248,   287,   491,
         1273,   310,  1067,   388,   297, 22473,   467,  5538,  5019,   505,
         7271,   411,   445,  2924,   310,  5925, 29973,  1334,   864,   304,
         1831,  8210,  4128,   363,  1269,   310,  1438,  3651,   313, 29872,
        29889, 29887,  1696,   825, 10809,   310,   975,  8399,  1145, 29892,
        10809,   304,  4094,  1591, 29892,  2992,  6250, 29871,   518, 29914,
        25580, 29962], device='cuda:0')]
Response Tensors:  [tensor([29889, 15993,   786, 29901, 20281, 14995, 27794,  5953, 30889,   660,
         6143, 29872,   398, 24709,   273,   498, 14995, 29906, 12729, 10943],
       device='cuda:0'), tensor([31872,     1,   718, 25214, 10541,   317, 20668,  7858,  5192,  2477,
         1516, 29888,  5019,  1123, 10457, 23134,  7967, 11082,  2025,  8446],
       device='cuda:0')]
Response Tensors:  2
Reward Logits:  tensor([0.6516], device='cuda:0')
Reward Logits:  tensor([0.9976], device='cuda:0')
Rewards List:  [tensor(0.6516, device='cuda:0'), tensor(0.9976, device='cuda:0')]
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Stats:  {'objective/kl': 0.0, 'objective/kl_dist': array([0., 0.], dtype=float32), 'objective/logprobs': array([[-22.02601  , -22.02601  , -22.02601  , ...,  -4.9557056,
        -10.674408 , -10.631025 ],
       [-22.02601  , -22.02601  , -22.02601  , ..., -10.8352375,
        -10.096785 ,  -9.651636 ]], dtype=float32), 'objective/ref_logprobs': array([[-22.02601  , -22.02601  , -22.02601  , ...,  -4.9557056,
        -10.674408 , -10.631025 ],
       [-22.02601  , -22.02601  , -22.02601  , ..., -10.8352375,
        -10.096785 ,  -9.651636 ]], dtype=float32), 'objective/kl_coef': 0.2, 'objective/entropy': 176.74256896972656, 'ppo/mean_non_score_reward': 0.0, 'ppo/mean_scores': 0.8246092200279236, 'ppo/std_scores': 0.2447112649679184, 'tokens/queries_len_mean': 512.0, 'tokens/queries_len_std': 0.0, 'tokens/queries_dist': array([512., 512.], dtype=float32), 'tokens/responses_len_mean': 20.0, 'tokens/responses_len_std': 0.0, 'tokens/responses_dist': array([20., 20.], dtype=float32), 'ppo/loss/policy': nan, 'ppo/loss/value': nan, 'ppo/loss/total': nan, 'ppo/policy/entropy': nan, 'ppo/policy/approxkl': nan, 'ppo/policy/policykl': nan, 'ppo/policy/clipfrac': 0.0, 'ppo/policy/advantages': array([-3.7328184, -3.7328184, -3.7328184, ...,  0.6484721,  1.9656414,
        1.1948361], dtype=float32), 'ppo/policy/advantages_mean': -1.8030405612989853e-07, 'ppo/policy/ratio': array([ 1.,  1.,  1., ..., nan, nan, nan], dtype=float32), 'ppo/returns/mean': 0.3291398286819458, 'ppo/returns/var': 0.07396463304758072, 'ppo/val/vpred': nan, 'ppo/val/error': nan, 'ppo/val/clipfrac': 0.02500000037252903, 'ppo/val/mean': -0.5738201141357422, 'ppo/val/var': 0.021302644163370132, 'ppo/time/ppo/optimizer_step': 0.025072216987609863, 'ppo/val/var_explained': nan, 'ppo/learning_rate': 2e-05, 'time/ppo/forward_pass': 1.087998390197754, 'time/ppo/compute_rewards': 0.0006344318389892578, 'time/ppo/optimize_step': 1.3271827697753906, 'time/ppo/calc_stats': 0.0020797252655029297, 'time/ppo/total': 2.4288036823272705}

  0%|          | 1/8713 [00:05<12:40:50,  5.24s/it][AQuery Tensors:  2
Query Tensors:  [tensor([    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     1,     2,     1,
          518, 25580, 29962,  3532, 14816, 29903,  6778,    13, 12148,  1234,
          278,  5155,  4475,   304,  1737,   359, 15277, 29889,    13, 29966,
          829, 14816, 29903,  6778,    13, 23323,   366,  6597,   278,  4958,
         4475,   304,  8437, 10466,  5276,   297,   445, 13382, 29973,   450,
        13382, 29901, 18375,  3245,   391,   278,  3002, 29892,  6041, 12992,
          304,   408,   278,  9193,   279, 30150,  3762, 29892,   589,  3145,
          967,  1024,   515,   278,  1734,   376, 29576,   654, 29908,   408,
          263, 13962,   310,   278, 10387,   293,  1734,   750,   389,   470,
          515,   278, 10387,   293,  1734, 17622,   279, 29892,  6593,   376,
        29876,  2749,   800,  1642,   450, 13807,   391,   907,   287,   338,
          304,  4772,   628,  1747,   964, 20607,   278,  5996,  1580,  2785,
        29889,  2688, 19104,   373,   278,   660,   332, 29915,   273, 29892,
          278,   317,  5963,   801, 29892,   322,  1827,   886,   310,   278,
        24246,  5363, 29892,  8790,   445,   408,   278,  7256,  2224,   988,
          278,  8393,   310,   838,  8083,   526,  9259,  1728,  1139,   292,
         1009,  5469,   313,  5365, 29899,   433,   413,   388, 29888,   467,
          319,  7184,   328, 29736,  7169,  5521,   338, 17878,   408,   278,
        11822,   310,   278, 13807,   391,  3762,   310,   907,   287, 29889,
          450,  5400,  3956,  2142, 29875, 10298,  4067,  1078,  3528,   411,
          278,  9193,   279, 30150,   907,   287, 29889, 29871,   518, 29914,
        25580, 29962], device='cuda:0'), tensor([    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     1,     2,     1,   518, 25580, 29962,
         3532, 14816, 29903,  6778,    13, 12148,  1234,   278,  5155,  4475,
          304,  1737,   359, 15277, 29889,    13, 29966,   829, 14816, 29903,
         6778,    13, 29902,   626,  3063,   363, 10529,   363,   263,  3918,
          982,   304, 16455,   344, 23585,  7048,  2073, 28337,   313, 15003,
          322,  5195, 29923,  6877,    13, 29902,   626,  1811,   304,  1106,
          714,   363,   263,   405,  9047,   470, 23801,   713,  3918,   363,
         7418,   310,  9637,   322,  5195, 29923,  3161,   297, 23585,  7048,
         2073, 28337, 29889,  4721, 16604,   471,   773,   341, 10051, 29899,
        29896, 10117,   515,   405,  9047,   541,  1286,   372,   338,   443,
        16515,   577,   738,  8671,   304,   393, 29889, 29871,   518, 29914,
        25580, 29962], device='cuda:0')]
  0%|          | 1/8713 [00:05<14:02:30,  5.80s/it]
epoch:   0%|          | 0/1 [00:05<?, ?it/s]
Traceback (most recent call last):
  File "/gpfs/bwfor/home/tu/tu_tu/tu_zxojp43/master_thesis/RLHF/ppo.py", line 184, in <module>
    build_pipeline(ppo_config, ppo_trainer, policy_tokenizer, reward_model, reward_tokenizer, dataloader)
  File "/gpfs/bwfor/home/tu/tu_tu/tu_zxojp43/master_thesis/RLHF/ppo.py", line 116, in build_pipeline
    response_tokens = ppo_trainer.generate(query, return_prompt=False, **generation_kwargs)
  File "/home/tu/tu_tu/tu_zxojp43/.local/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py", line 442, in generate
    response = self.accelerator.unwrap_model(self.model).generate(
  File "/home/tu/tu_tu/tu_zxojp43/.local/lib/python3.9/site-packages/trl/models/modeling_value_head.py", line 198, in generate
    return self.pretrained_model.generate(*args, **kwargs)
  File "/home/tu/tu_tu/tu_zxojp43/.local/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/tu/tu_tu/tu_zxojp43/.local/lib/python3.9/site-packages/transformers/generation/utils.py", line 1588, in generate
    return self.sample(
  File "/home/tu/tu_tu/tu_zxojp43/.local/lib/python3.9/site-packages/transformers/generation/utils.py", line 2678, in sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
