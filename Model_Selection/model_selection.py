"""
Model selection

* Randomly sample 20 queries of the SFT dataset
* Loading different mistral and llama models from huggingface
* Run inference with 20 these samples for each model
* Manually compare/evaluate the results
"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import pandas as pd
from datasets import load_dataset
from functools import reduce
import os
import argparse
import torch


# Data Preparation

def create_dataset(name, range, seed):
    """
    Load dataset and randomly sample a number of prompts from the dataset as well as their labels and additional input.
    Concatenate prompt and additional inputs.
    :param name: path to the dataset
    :param range: number of prompts that should be sampled
    :param seed: int number to guarantee reproduction of random sampling
    :return: gold_labels, prompts: a list with the labels sampled from the dataset and a list with the prompts
    """
    data = load_dataset(name, split="train")

    shuffled_dataset = data.shuffle(seed=seed)
    sampled_dataset = shuffled_dataset.select(range(range))

    raw_prompts = [sample["instruction"] for sample in sampled_dataset]
    input = [sample["input"] for sample in sampled_dataset]
    gold_labels = [sample["output"] for sample in sampled_dataset]
    prompts = reduce(lambda res, l: res + [l[0] + l[1] + " [EOS] "], zip(raw_prompts, input), [])

    return gold_labels, prompts


# Model

def load_model(model_name):
    """
    Loads the model and sets some config parameters
    :param model_name: path to where the model is stored
    :return: model, tokenizer: the model and the tokenizer object
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    # model = AutoModelForCausalLM.from_pretrained(model_name, device_map="cuda", torch_dtype=torch.float16)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        load_in_4bit=True,
        device_map="auto",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )

    model.config.pad_token_id = tokenizer.bos_token_id
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.bos_token_id
    tokenizer.padding_side = "right"

    return model, tokenizer


# Inference & Save

def inference(model, tokenizer, prompts, labels, max_new_tokens):
    """
    Run inference on prompts and return result lists.
    :param model: the model object
    :param tokenizer: the tokenizer object
    :param prompts: a list with the prompts taken from the dataset
    :param labels: a list with the labels taken from the dataset
    :param max_new_tokens: number of maximal tokens the model is allowed to generate
    :return: pred_list, input_list, label_list: list with the predictions, the prompts and the labels
    """
    # model.half()
    model.eval()
    # model.to("cuda")

    pred_list = []
    input_list = []
    label_list = []

    # inference
    with torch.no_grad():
        for post, label in zip(prompts, labels):
            encode_dict = tokenizer(post, return_tensors="pt", padding=True, truncation=True)
            txt_tokens = encode_dict["input_ids"].cuda()
            attention_mask = encode_dict["attention_mask"].cuda()
            kwargs = {"max_new_tokens": max_new_tokens, "eos_token_id": 50256, "pad_token_id": 50256}
            summ_tokens = model.generate(txt_tokens, attention_mask=attention_mask, **kwargs)
            pred = tokenizer.batch_decode(summ_tokens)[0]
            pred = pred.split("[EOS]")[1].split(tokenizer.eos_token)[0].replace("<|endoftext|>", "")
            pred_list.append(pred)
            input_list.append(post.replace(" [EOS]", ""))
            label_list.append(label.replace("\n", " "))

    torch.cuda.empty_cache()
    return pred_list, input_list, label_list


def save_to_csv(pred_list, gold_list, input_list, output_dir, output_filename):
    """
    Saves the results in a csv file.
    :param pred_list: a list with the predictions generated by the model
    :param gold_list: a list with the labels taken from the dataset
    :param input_list: a list with the prompts taken from the dataset
    :param output_dir: path to a directory where the result files should be stored
    :param output_filename: name for the result file
    :return: -
    """

    df = pd.DataFrame.from_dict(
        {
            "input": input_list,
            "gold": gold_list,
            f"pred_{args.model_name}": pred_list,
        }
    )

    output_path = os.path.join(output_dir, output_filename)
    df.to_csv(output_path, index=False)


if __name__ == "__main__":
    # Args from sh script
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name", type=str, required=True)
    args = parser.parse_args()

    # Variables
    dataset = "daven3/geosignal"
    count_samples = 20
    seed = 42
    system_input = ""
    max_new_tokens = 200
    output_dir = "../Model_Selection/Output_files/"
    model_saving_path = args.model_name.replace("/", "-")
    output_filename = f"results_{model_saving_path}_{count_samples}samples_{seed}seed.csv"

    # Functions
    gold_labels, raw_prompts = create_dataset(dataset, count_samples, seed)
    print("create_dataset() done!")
    model, tokenizer = load_model(args.model_name)
    print("load_model() done!")
    predictions, prompts, labels = inference(model, tokenizer, raw_prompts, gold_labels, max_new_tokens)
    print("inference() done!")
    save_to_csv(predictions, gold_labels, prompts, output_dir, output_filename)
    print("save_to_csv() done!")
