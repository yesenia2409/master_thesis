"""
SFT: inference

* Load the geosignal json file from GitHub
* Concatenate instruction and input columns
* Apply the prompt template to each prompt
* Save the dataset as pickle and csv files
"""

import repackage
repackage.up()

from peft import PeftModel
import torch
import os
import pandas as pd
from Prompting import proof_of_concept


def inference(model, tokenizer, prompts, labels, max_new_tokens):
    """
    Run inference on prompts and return result lists.
    :param model: the model object
    :param tokenizer: the tokenizer object
    :param prompts: a list with the prompts taken from the dataset
    :param labels: a list with the labels taken from the dataset
    :param max_new_tokens: number of maximal tokens the model is allowed to generate
    :return: pred_list, input_list, label_list: list with the predictions, the prompts and the labels
    """
    # model.half()
    model.eval()
    # model.to("cuda")
    # model = model.merge_and_unload()

    pred_list = []
    input_list = []
    label_list = []

    for idx, prompt in enumerate(prompts):
        prompts[idx] = f"<s> [INST] <<SYS>> \n You are a helpful, respectful and honest assistant. Always answer as helpfully as " \
           f"possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, " \
           f"dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in " \
           f"nature. If a question does not make any sense, or is not factually coherent, explain why instead of " \
           f"answering something not correct. If you don’t know the answer to a question, please don’t share false " \
           f"information. \n Please answer the questions related to geoscience. \n <</SYS>> \n {prompt} [/INST] </s>"

    # inference
    with torch.no_grad():
        for prompt, label in zip(prompts, labels):
            encode_dict = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
            txt_tokens = encode_dict["input_ids"].cuda()
            attention_mask = encode_dict["attention_mask"].cuda()
            kwargs = {"max_new_tokens": max_new_tokens, "eos_token_id": 50256, "pad_token_id": 50256}
            summ_tokens = model.generate(input_ids=txt_tokens, attention_mask=attention_mask, **kwargs)
            pred = tokenizer.batch_decode(summ_tokens)[0]
            pred = pred.split("[EOS]")[1].split(tokenizer.eos_token)[0].split("[/EOS]")[0].replace("<|endoftext|>", "")
            pred_list.append(pred)
            input_list.append(prompt.replace(" [EOS]", ""))
            label_list.append(label.replace("\n", " "))
    return pred_list, input_list, label_list


def save_to_csv(pred_list, gold_list, input_list, output_path):
    """
    Saves the results in a csv file.
    :param pred_list: a list with the predictions generated by the model
    :param gold_list: a list with the labels taken from the dataset
    :param input_list: a list with the prompts taken from the dataset
    :param output_path: path to a file where the result files should be stored
    :return: -
    """

    df = pd.DataFrame.from_dict(
        {
            "input": input_list,
            "gold": gold_list,
            "pred": pred_list,
        }
    )
    df.to_csv(output_path, index=False)


if __name__ == "__main__":
    # Variables
    dataset = "daven3/geosignal"
    base_model = "meta-llama/Llama-2-13b-chat-hf"
    model_dir_local = "Model/SFT_for_expert_alignment/"
    count_samples = 25
    seed = 33
    max_new_tokens = 521
    output_dir = "Output_files/inference_tests/"
    output_filename = f"inference_test_{count_samples}samples_{seed}seed_1epoch_2batch_2_00E-5_allLinearLayers_withTemplate_afterHumanAlignment.csv"
    output_path = os.path.join(output_dir, output_filename)

    # Functions
    labels, prompts = proof_of_concept.create_dataset(dataset, count_samples, seed)
    print("create_dataset() done!")
    model, tokenizer = proof_of_concept.load_model(base_model)
    print("load_model() done!")
    model = PeftModel.from_pretrained(model, model_dir_local)

    pred_list, input_list, label_list = inference(model, tokenizer, prompts, labels, max_new_tokens)
    print("inference() done!")
    save_to_csv(pred_list, label_list, input_list, output_path)
    print("save_to_csv() done!")
