"""
SFT: inference

* Load the geosignal json file from GitHub
* Concatenate instruction and input columns
* Apply the prompt template to each prompt
* Save the dataset as pickle and csv files
"""

import repackage
repackage.up()

from peft import PeftModel
import torch
import os
import pandas as pd
from Prompting import proof_of_concept


def inference(model, tokenizer, prompts, labels, max_new_tokens):
    """
    Run inference on prompts and return result lists.
    :param model: the model object
    :param tokenizer: the tokenizer object
    :param prompts: a list with the prompts taken from the dataset
    :param labels: a list with the labels taken from the dataset
    :param max_new_tokens: number of maximal tokens the model is allowed to generate
    :return: pred_list, input_list, label_list: list with the predictions, the prompts and the labels
    """
    # model.half()
    model.eval()
    # model.to("cuda")
    # model = model.merge_and_unload()

    pred_list = []
    input_list = []
    label_list = []

    # inference
    with torch.no_grad():
        for prompt, label in zip(prompts, labels):
            encode_dict = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
            txt_tokens = encode_dict["input_ids"].cuda()
            attention_mask = encode_dict["attention_mask"].cuda()
            kwargs = {"max_new_tokens": max_new_tokens, "eos_token_id": 50256, "pad_token_id": 50256}
            summ_tokens = model.generate(input_ids=txt_tokens, attention_mask=attention_mask, **kwargs)
            pred = tokenizer.batch_decode(summ_tokens)[0]
            pred = pred.split("[EOS]")[1].split(tokenizer.eos_token)[0].replace("<|endoftext|>", "")
            pred_list.append(pred)
            input_list.append(prompt.replace(" [EOS]", ""))
            label_list.append(label.replace("\n", " "))

    return pred_list, input_list, label_list


def save_to_csv(pred_list, gold_list, input_list, output_path):
    """
    Saves the results in a csv file.
    :param pred_list: a list with the predictions generated by the model
    :param gold_list: a list with the labels taken from the dataset
    :param input_list: a list with the prompts taken from the dataset
    :param output_path: path to a file where the result files should be stored
    :return: -
    """

    df = pd.DataFrame.from_dict(
        {
            "input": input_list,
            "gold": gold_list,
            "pred": pred_list,
        }
    )
    df.to_csv(output_path, index=False)


if __name__ == "__main__":
    # Variables
    dataset = "daven3/geosignal"
    base_model = "meta-llama/Llama-2-13b-chat-hf"
    model_dir_local = "Model/SFT_for_human_alignment/"
    count_samples = 25
    seed = 33
    max_new_tokens = 521
    output_dir = "Output_files/"
    output_filename = f"inference_test_{count_samples}samples_{seed}seed_1epoch_2batch_001Lr_allLinearLayers.csv"
    output_path = os.path.join(output_dir, output_filename)

    # Functions
    labels, prompts = proof_of_concept.create_dataset(dataset, count_samples, seed)
    print("create_dataset() done!")
    model, tokenizer = proof_of_concept.load_model(base_model)
    print("load_model() done!")
    model = PeftModel.from_pretrained(model, model_dir_local)

    pred_list, input_list, label_list = inference(model, tokenizer, prompts, labels, max_new_tokens)
    print("inference() done!")
    save_to_csv(pred_list, label_list, input_list, output_path)
    print("save_to_csv() done!")
